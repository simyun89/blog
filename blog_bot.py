import os, re, time, requests, pandas as pd
import textwrap, json, tiktoken
from datetime import datetime, timedelta
from collections import Counter
from konlpy.tag import Okt
import base64
import openai

confluence_api_token = os.environ.get("CONFLUENCE_API_TOKEN")
confluence_api_user = os.environ.get("CONFLUENCE_API_USER")
CID  = os.environ.get("NAVER_CLIENT_ID")
CSEC = os.environ.get("NAVER_CLIENT_SECRET")

openai_api_key = os.environ.get("OPENAI_API_KEY")
client = openai.OpenAI(api_key=openai_api_key)

HEAD = {'X-Naver-Client-Id': CID, 'X-Naver-Client-Secret': CSEC}
URL  = 'https://openapi.naver.com/v1/search/blog.json'

space_key = 'CSO'
parent_page_id = '661848065'
title = f"ì£¼ê°„ ë¸”ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ë¦¬í¬íŠ¸_{datetime.now():%Y-%m-%d}"
keywords = [ '"ì´ì¦ êµí†µì¹´ë“œ"', '"ezl"',  '"ì´ì¦"', '"ìºì‹œë¹„"', '"ì´ë™ì˜ì¦ê±°ì›€"', '"í‹°ë¨¸ë‹ˆ"']   # ì›í•˜ëŠ” ë§Œí¼ ì¶”ê°€
mapping  = {'ì´ë™ì˜ì¦ê±°ì›€':'ìì‚¬', 'ì´ì¦':'ìì‚¬', 'ezl':'ìì‚¬', 'ì´ì¦ êµí†µì¹´ë“œ':'ìì‚¬', 'ìºì‹œë¹„':'ìì‚¬',
            'í‹°ë¨¸ë‹ˆ':'ê²½ìŸì‚¬'}
end_date   = (datetime.now() - timedelta(days=1)).date()
start_date = (datetime.now() - timedelta(days=7)).date()
kw_order = [kw.strip('"') for kw in keywords]
okt = Okt() 

# ë¸”ë¡œê·¸ ë°ì´í„° ìˆ˜ì§‘
detail_rows = []
for kw in keywords:
    kw_clean = kw.strip('"')
    start = 1
    while start <= 1000:
        js = requests.get(URL,
                          params={'query': kw, 'display':100,
                                  'start':start, 'sort':'date'},
                          headers=HEAD).json()
        if 'items' not in js: break

        for it in js['items']:
            pdate = datetime.strptime(it['postdate'], '%Y%m%d').date()
            if pdate < start_date:
                start = 1001; break
            if pdate > end_date:
                continue
            detail_rows.append({
                'êµ¬ë¶„'   : mapping[kw_clean],
                'í‚¤ì›Œë“œ' : kw_clean,
                'ë‚ ì§œ'   : pdate.strftime('%Y-%m-%d'),
                'ì œëª©'   : re.sub('<.*?>', '', it['title']),
                'URL'    : it['link']
            })

        if js.get('display', 0) < 100 or start >= 1000:
            break
        start += 100

detail_df = (pd.DataFrame(detail_rows)
             .drop_duplicates('URL')         # (URL ì¤‘ë³µ ì œê±°) ì—¬ëŸ¬ URL ì¤‘ë³µì´ ìˆì„ ì‹œ 1ê±´ë§Œ ë‚¨ê²¨ë‘  í‚¤ì›Œë“œ ìˆœìœ¼ë¡œ ë‚¨ê²¨ë‘ 
             .reset_index(drop=True))

# â”€â”€ 2. ìš”ì•½ í…Œì´ë¸” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2-a) êµ¬ë¶„ë³„ ìš”ì•½
summary_df = (detail_df.groupby('êµ¬ë¶„', observed=True)
                        .size()
                        .reset_index(name='ìµœê·¼ 7ì¼')
                        .sort_values('êµ¬ë¶„'))

# 2-b) êµ¬ë¶„ë³„ x í‚¤ì›Œë“œ ìš”ì•½
summary_kw_df = (detail_df.assign(
                     í‚¤ì›Œë“œ=pd.Categorical(detail_df['í‚¤ì›Œë“œ'],
                                           categories=kw_order, ordered=True))
                 .groupby(['êµ¬ë¶„','í‚¤ì›Œë“œ'], observed=True)
                 .size()
                 .reset_index(name='ìµœê·¼ 7ì¼')
                 .sort_values(['êµ¬ë¶„','í‚¤ì›Œë“œ']))

# 2-c) ì¼ìë³„ x í‚¤ì›Œë“œë³„ ê±´ìˆ˜
daily_df = (detail_df.groupby(['ë‚ ì§œ','í‚¤ì›Œë“œ'])
                      .size()
                      .unstack(fill_value=0)
                      .reindex(columns=kw_order, fill_value=0)
                      .reindex(pd.date_range(start_date, end_date)
                                  .strftime('%Y-%m-%d'), fill_value=0))


# 2-d) ë‹¨ì–´ ë¹ˆë„
stop = {'ì´ì¦', 'ezl', 'í‹°ë¨¸ë‹ˆ', 'ìºì‹œë¹„'} # ì œì™¸í•  ë‹¨ì–´
ja_cn, co_cn = Counter(), Counter()
for row in detail_df[['êµ¬ë¶„', 'ì œëª©']].itertuples(index=False):
    tokens = [w for w, pos in okt.pos(row.ì œëª©, stem=True)
              if pos in ('Noun','Adjective') and len(w) > 1 and w not in stop]
    (ja_cn if row.êµ¬ë¶„ == 'ìì‚¬' else co_cn).update(tokens)

N = 20                                    # Top N ì¶”ì¶œ
ja_top = ja_cn.most_common(N)
co_top = co_cn.most_common(N)
max_len = max(len(ja_top), len(co_top))
ja_top += [('', 0)] * (max_len - len(ja_top))
co_top += [('', 0)] * (max_len - len(co_top))
freq_side_df = pd.DataFrame({
    'ë‹¨ì–´(ìì‚¬)' : [w for w, _ in ja_top],
    'ê±´ìˆ˜(ìì‚¬)'  : [c for _, c in ja_top],
    'ë‹¨ì–´(ê²½ìŸì‚¬)': [w for w, _ in co_top],
    'ê±´ìˆ˜(ê²½ìŸì‚¬)' : [c for _, c in co_top],
})

# 2-e) GPTë¡œ ì´ìŠˆ í´ëŸ¬ìŠ¤í„°ë§
enc = tiktoken.encoding_for_model("gpt-4o-mini")
def num_tokens(txt): return len(enc.encode(txt))

def gpt_cluster(title_list, label):
    titles_block = "\n".join(f"- {t}" for t in title_list)
    if num_tokens(titles_block) > 7000:
        titles_block = "\n".join(f"- {t}" for t in title_list[:600])

    prompt = textwrap.dedent(f"""
      ì•„ë˜ëŠ” ìµœê·¼ 7ì¼ê°„ **{label}** ë¸”ë¡œê·¸ ê¸€ ì œëª© ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.

      {titles_block}

      ë‹¤ìŒ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
      1) ì˜ë¯¸ê°€ ìœ ì‚¬í•˜ê±°ë‚˜ ë°˜ë³µë˜ëŠ” ì œëª©ë¼ë¦¬ ë¬¶ì–´ì„œ, 'ì´ìŠˆ ìœ í˜•'ì„ ìµœëŒ€ 10ê°œê¹Œì§€ ë„ì¶œí•˜ì„¸ìš”.
      2) ê° ì´ìŠˆ ìœ í˜•ë³„ë¡œ ì œëª© ê±´ìˆ˜ë¥¼ ì„¸ì„œ, ["ì´ìŠˆ ìœ í˜•", ê±´ìˆ˜] í˜•íƒœì˜ JSON ë°°ì—´ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
      3) ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì€ í•„ë“œëª…ë§Œ ì‚¬ìš©í•˜ì„¸ìš”: "type" (ì´ìŠˆ ìœ í˜•), "count" (ê±´ìˆ˜)
      4) ì„¤ëª…, í•´ì„¤, ì¶”ê°€ ì½”ë©˜íŠ¸, ì˜ˆì‹œ ë“±ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
      
      ì˜¤ì§ JSON ë°ì´í„°ë§Œ ê²°ê³¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
    """).strip()

    resp = openai.chat.completions.create(
        model="gpt-4.1",
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )

    raw = resp.choices[0].message.content.strip()
    raw = re.sub(r"^```(?:json)?|```$", "", raw, flags=re.I|re.M).strip()
    raw = raw[raw.find('['): raw.rfind(']') + 1]   # '[' ... ']' ì‚¬ì´ë§Œ

    issue_df = pd.DataFrame(json.loads(raw))
    issue_df.columns = [c.lower() for c in issue_df.columns]

    if 'count' not in issue_df.columns:
        num_cols = [c for c in issue_df.columns if pd.api.types.is_numeric_dtype(issue_df[c])]
        if num_cols:
            issue_df = issue_df.rename(columns={num_cols[0]: 'count'})

    if 'type' not in issue_df.columns:
        cat_cols = [c for c in issue_df.columns if not pd.api.types.is_numeric_dtype(issue_df[c]) and c != 'êµ¬ë¶„']
        if cat_cols:
            issue_df = issue_df.rename(columns={cat_cols[0]: 'type'})
    issue_df.insert(0, 'êµ¬ë¶„', label)   # ìì‚¬/ê²½ìŸì‚¬ í‘œì‹œ
    return issue_df

titles_jasa = detail_df.query("êµ¬ë¶„ == 'ìì‚¬'")['ì œëª©'].tolist()   #  ìì‚¬Â·ê²½ìŸì‚¬ ê°ê° í˜¸ì¶œ
titles_comp = detail_df.query("êµ¬ë¶„ == 'ê²½ìŸì‚¬'")['ì œëª©'].tolist()

issue_df_jasa = gpt_cluster(titles_jasa, 'ìì‚¬')
issue_df_comp = gpt_cluster(titles_comp, 'ê²½ìŸì‚¬')

issue_df_all = pd.concat([issue_df_jasa, issue_df_comp], ignore_index=True)

def make_bullet(df):     #  ë¦¬í¬íŠ¸ ë¬¸êµ¬
    return "\n".join(
        f"- **{row['type']}** : {row['count']}ê±´"
        for _, row in df.sort_values('count', ascending=False).iterrows()
    )
print("\n[ìì‚¬ ì´ìŠˆ Top]\n", make_bullet(issue_df_jasa))
print("\n[ê²½ìŸì‚¬ ì´ìŠˆ Top]\n", make_bullet(issue_df_comp))


# ê²°ê³¼ í™•ì¸ / ì €ì¥
pd.set_option('display.max_colwidth', None)
detail_df.to_csv('blog_detail.csv',  index=False, encoding='utf-8-sig')

# â”€â”€ 3. ë¦¬í¬íŠ¸ìš© HTML Â· ë³¸ë¬¸ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
summary_html      = summary_df.fillna('').to_html(index=False, border=1)
summary_kw_html   = summary_kw_df.to_html(index=False, border=1)    # fillna('') X
daily_html        = daily_df.fillna('').reset_index(names='ë‚ ì§œ').to_html(index=False, border=1)
freq_html         = freq_side_df.fillna('').to_html(index=False, border=1)
issue_jasa_tbl_html = (
    issue_df_jasa.fillna('').rename(columns={'type': 'ì´ìŠˆ ìœ í˜•', 'count': 'ê±´ìˆ˜'})
                 .to_html(index=False, border=1)
)
issue_comp_tbl_html = (
    issue_df_comp.fillna('').rename(columns={'type': 'ì´ìŠˆ ìœ í˜•', 'count': 'ê±´ìˆ˜'})
                 .to_html(index=False, border=1)
)

# 3-b) Confluence ë³¸ë¬¸ í…œí”Œë¦¿
body_html = f"""
<br><h2>ğŸ“ ì°¸ê³  ì‚¬í•­</h2>
<ul>
  <li>ë³¸ ë³´ê³ ì„œëŠ” ë§¤ì£¼ ì›”ìš”ì¼ ì˜¤ì „ 7ì‹œì— ìë™ ë°œì†¡ë©ë‹ˆë‹¤.</li>
  <li>ì§€ë‚œì£¼(ì›”~ì¼) ë„¤ì´ë²„ ë¸”ë¡œê·¸ì— ê²Œì‹œëœ ê¸€ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.</li>
  <li>ë¶„ì„ì— ì‚¬ìš©ëœ ê²€ìƒ‰ í‚¤ì›Œë“œ : {' , '.join(kw_order)}</li>
  <li>ë™ì¼í•œ URLì€ ì¤‘ë³µì„ ì œê±°í•˜ì—¬ ì§‘ê³„í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>

<br><h2>ğŸ“Š 1. êµ¬ë¶„ë³„ ë¸”ë¡œê·¸ ì–¸ê¸‰ ê±´ìˆ˜</h2>
{summary_html}

<br><h2>ğŸ“‘ 2. êµ¬ë¶„ Ã— í‚¤ì›Œë“œë³„ ì–¸ê¸‰ ê±´ìˆ˜</h2>
{summary_kw_html}

<br><h2>ğŸ“ˆ 3. ì¼ìë³„ í‚¤ì›Œë“œ íŠ¸ë Œë“œ</h2>
{daily_html}

<br><h2>ğŸ—£ï¸ 4. ë‹¨ì–´ ë¹ˆë„(Top 20)</h2>
{freq_html}

<br><h2>ğŸ’¡ 5. AI ê¸°ë°˜ ë¸”ë¡œê·¸ ì´ìŠˆ ìœ í˜• ì •ë¦¬</h2>
<ul>
  <li>GPTë¡œ ë¸”ë¡œê·¸ ê¸€ì„ ë¶„ì„í•˜ì—¬ ì£¼ìš” ì´ìŠˆë¥¼ ìœ í˜•ë³„ë¡œ ì •ë¦¬í•œ í‘œì…ë‹ˆë‹¤.</li>
</ul>
<h3>â‘  ìì‚¬ TOP ì´ìŠˆ</h3>
{issue_jasa_tbl_html}
<br><h3>â‘¡ ê²½ìŸì‚¬ TOP ì´ìŠˆ</h3>
{issue_comp_tbl_html}

<br><h2>ğŸ“¥ [Raw Data] ë¸”ë¡œê·¸ ìƒì„¸ ë‚´ì—­ ë‹¤ìš´ë¡œë“œ</h2>
"""

# â”€â”€ 4. Confluence í˜ì´ì§€ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
confluence_domain = 'myezl.atlassian.net'
headers = {
    'Authorization': 'Basic ' +
        base64.b64encode(f'{confluence_api_user}:{confluence_api_token}'
                         .encode()).decode(),
    'Content-Type': 'application/json'
}
base_url = f'https://{confluence_domain}/wiki/rest/api/content/'

page_data = {
    "type": "page",
    "title": title,
    "ancestors": [{"id": parent_page_id}],
    "space": {"key": space_key},
    "body": {"storage": {"value": body_html, "representation": "storage"}}
}
resp = requests.post(base_url, headers=headers, json=page_data)
resp.raise_for_status()
page_id = resp.json()['id']
print("í˜ì´ì§€ ìƒì„± âœ”", page_id)

# â”€â”€ 5. CSV ì²¨ë¶€ & ë§í¬ ì‚½ì… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
attach_url  = f"{base_url}{page_id}/child/attachment"
attach_head = {
    "Authorization": headers['Authorization'],
    "X-Atlassian-Token": "no-check",
}
with open('blog_detail.csv', 'rb') as fp:
    files = {"file": ("blog_detail.csv", fp, "text/csv")}
    aresp = requests.post(attach_url, headers=attach_head, files=files)
aresp.raise_for_status()
fname = aresp.json()['results'][0]['title']
print("ì²¨ë¶€ âœ”", fname)

# í˜ì´ì§€ ë²„ì „ ì˜¬ë ¤ì„œ ì²¨ë¶€ ë§í¬(ri:attachment) ì‚½ì…
ver = requests.get(f"{base_url}{page_id}?expand=version",
                   headers=headers).json()['version']['number']
patch = {
    "version": {"number": ver + 1, "minorEdit": True},
    "title"  : page_data["title"],
    "type"   : "page",
    "body"   : {
        "storage": {
            "value": body_html + f'<p><ac:link><ri:attachment '
                      f'ri:filename="{fname}" /></ac:link></p>',
            "representation": "storage"
        }
    }
}
requests.put(f"{base_url}{page_id}", headers=headers, json=patch
             ).raise_for_status()
print("ë³¸ë¬¸ì— ì²¨ë¶€ ë§í¬ ì¶”ê°€ âœ”")



